{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, LayerNormalization, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Neutral': 0,\n",
       " 'Finish': -1,\n",
       " 'Close': 1,\n",
       " 'Indication': 8,\n",
       " 'Open': 2,\n",
       " 'Pinch': 7,\n",
       " 'ThumbFingers': 6,\n",
       " 'Wrist_Extend': 4,\n",
       " 'Wrist_Flex': 3,\n",
       " 'Baseline': -1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocol = pd.read_csv('data/new/2024-12-09_11-22-43.emg8.protocol', sep=',', index_col=0)\n",
    "LABELS = dict(protocol[['State', 'ID']].value_counts().index)\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNC_COL = 'sample'   # порядковый номер размеченного жеста - для синхронизации и группировок\n",
    "TARGET = 'act_label'  # таргет - метка фактически выполняемого жеста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMG_CHANELS_CNT = 16  # количество каналов OMG-датчиков\n",
    "OMG_COL_PRFX = 'omg'  # префикс в названиях столбцов датафрейма, соответствующих OMG-датчикам\n",
    "LABEL_COL = 'id'      # столбец таргета\n",
    "TS_COL = 'ts'         # столбец метки времени\n",
    "\n",
    "SYNC_COL = 'sample'   # порядковый номер размеченного жеста - для синхронизации и группировок\n",
    "TARGET = 'act_label'  # таргет - метка фактически выполняемого жеста\n",
    "\n",
    "MODEL_PATH = 'model/lstm_model.h5'\n",
    "\n",
    "# Сформируем список с названиями всех столбцов OMG\n",
    "OMG_CH = [OMG_COL_PRFX + str(i) for i in range(OMG_CHANELS_CNT)]\n",
    "# print('OMG chanels:', ', '.join(OMG_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для чтения данных\n",
    "def read_emg8(\n",
    "        montage: str,\n",
    "        dir: str = 'data',\n",
    "        sep: str = ',',\n",
    "        drop_baseline_and_finish: bool = True,\n",
    "        omg_only: bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Осуществляет чтение файла с данными измерений монтажа .emg8.\n",
    "\n",
    "    Добавляет в возвращаемый датафрейм признак `sample`, представляющий собой порядковый номер жеста в монтаже.\n",
    "\n",
    "    ### Параметры\n",
    "\n",
    "    **montage**: *str*<br>\n",
    "    Имя файла для чтения\n",
    "\n",
    "    **dir**: *str, default=\"data\"*<br>\n",
    "    Название папки, в которой находится файл\n",
    "\n",
    "    **sep**: *str, default=' '*<br>\n",
    "    Символ-разделитель в csv-файле\n",
    "\n",
    "    **drop_baseline_and_finish**: *bool, default=True*<br>\n",
    "    Удалять ли в начале и в конце монтажа измерения с метками `Baseline` и `Finish` соответственно\n",
    "\n",
    "    **omg_only**: *bool, default=False*<br>\n",
    "    Читать только столбцы датчиков OMG (для подгрузки тестовых данных)\n",
    "\n",
    "    ### Возвращаемый результат\n",
    "\n",
    "    **data**: *DataFrame*<br>\n",
    "    Датафрейм с прочитанными данными\n",
    "    '''\n",
    "    path = os.path.join(dir, montage)\n",
    "    cols = OMG_CH if omg_only else OMG_CH + [LABEL_COL, TS_COL]\n",
    "    data = pd.read_csv(path, sep=sep, index_col=None)[cols]\n",
    "\n",
    "    if not omg_only:\n",
    "        if drop_baseline_and_finish:\n",
    "            mask = (data[LABEL_COL] != LABELS['Finish']) & (data[LABEL_COL] != LABELS['Baseline'])\n",
    "            data = data[mask].reset_index(drop=True)\n",
    "\n",
    "        bounds = data[data[LABEL_COL] != data[LABEL_COL].shift(1)].index\n",
    "\n",
    "        for i, lr in enumerate(zip(bounds, np.append(bounds[1:], [data.index[-1]]))):\n",
    "            l, r = lr  # l, r - индексы начала текущей и следующей эпохи соответственно\n",
    "            data.loc[l: r, SYNC_COL] = i\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для разметки данных\n",
    "class BasePeakMarker(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Класс-преобразователь для добавления в данные признака `\"act_label\"` – метки фактически выполняемого жеста.\n",
    "\n",
    "    ### Параметры объекта класса\n",
    "\n",
    "    **sync_col**: *str, default=SYNC_COL*<br>\n",
    "    Название столбца с порядковым номером жеста (в соответствии с поступившей командой)\n",
    "\n",
    "    **label_col**: *str, default=LABEL_COL*<br>\n",
    "    Название столбца с меткой жеста (в соответствии с поступившей командой)\n",
    "\n",
    "    **ts_col**: *str, default=TS_COL*<br>\n",
    "    Название столбца с меткой времени\n",
    "\n",
    "    **hi_val_threshold**: *float, default=0.1*<br>\n",
    "    Нижний порог медианы показаний датчика (в нормализованных измерениях)\n",
    "    для отнесения его к числу *активных* датчиков\n",
    "\n",
    "    **sudden_drop_threshold**: *float, default=0.1*<br>\n",
    "    Верхний порог отношения первого процентиля к медиане показаний датчика\n",
    "    для отнесения его к числу датчиков, которым свойственны внезапные падения сигнала\n",
    "    до околонулевых значений (такие датчики игнорируются при выполнении разметки)\n",
    "\n",
    "    **sync_shift**: *int, default=0*<br>\n",
    "    Общий сдвиг меток синхронизации (признак `sync_col`)\n",
    "\n",
    "    **bounds_shift**: *int, default=0*<br>\n",
    "    Корректировка (сдвиг) найденных границ\n",
    "\n",
    "    **clean_w**: *int, default=5*<br>\n",
    "    Параметр используемый при очистке найденных максимумов (пиков):\n",
    "    если два или более пиков находятся на расстоянии не более `clean_w` измерений друг от друга,\n",
    "    то из них оставляем один самый высокий пик\n",
    "\n",
    "    **use_grad2**: *bool, default=True*<br>\n",
    "    Если True - алгоритм разметки использует локальные максимумы (пики) суммарного второго градиента\n",
    "    показаний датчиков. Иначе - используется градиент суммарного стандартного отклонения\n",
    "\n",
    "    ## Методы\n",
    "    Данный класс реализует стандартные методы классов-преобразователей *scikit-learn*:\n",
    "\n",
    "    `fit()`, `fit_transform()` и `transform()` и может быть использован как элемент пайплайна.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sync_col: str = SYNC_COL,\n",
    "        label_col: str = LABEL_COL,\n",
    "        ts_col: str = TS_COL,\n",
    "        hi_val_threshold: float = 0.1,\n",
    "        sudden_drop_threshold: float = 0.1,\n",
    "        sync_shift: int = 0,\n",
    "        bounds_shift: int = 0,\n",
    "        clean_w: int = 5,\n",
    "        use_grad2: bool = True\n",
    "    ):\n",
    "        self.sync_col = sync_col\n",
    "        self.label_col = label_col\n",
    "        self.ts_col = ts_col\n",
    "        self.hi_val_threshold = hi_val_threshold\n",
    "        self.sudden_drop_threshold = sudden_drop_threshold\n",
    "        self.sync_shift = sync_shift\n",
    "        self.bounds_shift = bounds_shift\n",
    "        self.clean_w = clean_w\n",
    "        self.use_grad2 = use_grad2\n",
    "\n",
    "    # Внутренний метод для нахождения \"пиков\" (второго градиента, градиента стандартного отклонения)\n",
    "    def _find_peaks(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        window: int,\n",
    "        spacing: int,\n",
    "    ):\n",
    "        def _peaks(arr):\n",
    "            mask = np.hstack([\n",
    "                [False],\n",
    "                (arr[: -2] < arr[1: -1]) & (arr[2:] < arr[1: -1]),\n",
    "                [False]\n",
    "            ])\n",
    "            peaks = arr.copy()\n",
    "            peaks[~mask] = 0\n",
    "            peaks[peaks < 0] = 0\n",
    "            return peaks\n",
    "\n",
    "        def _clean_peaks(arr, w=self.clean_w):\n",
    "            peaks = arr.copy()\n",
    "            # Разберемся с пиками, расположенными близко друг к другу:\n",
    "            # из нескольких пиков, помещающихся в окне w,\n",
    "            # оставим только один - максимальный\n",
    "            for i in range(peaks.shape[0] - w + 1):\n",
    "                slice = peaks[i: i + w]\n",
    "                max_peak = np.max(slice)\n",
    "                mask = slice != max_peak\n",
    "                peaks[i: i + w][mask] = 0\n",
    "            return peaks\n",
    "\n",
    "        # 1) Градиенты векторов показаний датчиков\n",
    "        grad1 = np.sum(np.abs(np.gradient(X, spacing, axis=0)), axis=1)\n",
    "        # не забудем заполнить образовавшиеся \"дырки\" из NaN\n",
    "        grad1 = np.nan_to_num(grad1)\n",
    "\n",
    "        grad2 = np.gradient(grad1, spacing, axis=0)\n",
    "        grad2 = np.nan_to_num(grad2)\n",
    "        peaks2 = _peaks(grad2)\n",
    "\n",
    "        # 2) Среднее стандартное отклонение и его градиент\n",
    "        std = np.mean(pd.DataFrame(X).rolling(window, center=True).std(), axis=1)\n",
    "        std = np.nan_to_num(std)\n",
    "\n",
    "        std1 = np.gradient(std, 1, axis=0)\n",
    "        std1 = np.nan_to_num(std1)\n",
    "        peaks_std1 = _peaks(std1)\n",
    "\n",
    "        # Возвращаем пики градиента стандартного отклонения и второго градиента\n",
    "        return _clean_peaks(peaks_std1), _clean_peaks(peaks2)\n",
    "\n",
    "    # Функция для непосредственной разметки\n",
    "    def _mark(\n",
    "        self,\n",
    "        X: pd.DataFrame\n",
    "    ) -> np.ndarray[int]:\n",
    "\n",
    "        # Сглаживание\n",
    "        X_omg = pd.DataFrame(X[self.mark_sensors]).rolling(self.window, center=True).median()\n",
    "        # Приведение к единому масштабу\n",
    "        X_omg = MinMaxScaler((1, 1000)).fit_transform(X_omg)\n",
    "\n",
    "        peaks_std1, peaks_grad2 = self._find_peaks(\n",
    "            X_omg,\n",
    "            window=self.window,\n",
    "            spacing=self.spacing\n",
    "        )\n",
    "\n",
    "        peaks = peaks_grad2 if self.use_grad2 else peaks_std1\n",
    "\n",
    "        sync = X[self.sync_col].copy()\n",
    "        # Сдвигаем синхронизацию\n",
    "        if self.sync_shift > 0:\n",
    "            sync.iloc[self.sync_shift:] = sync.iloc[: -self.sync_shift]\n",
    "            sync.iloc[: self.sync_shift] = 0\n",
    "\n",
    "        # Искать максимальные пики будем внутри отрезков,\n",
    "        # определяемых по признаку синхронизации\n",
    "        sync_mask = sync != sync.shift(-1)\n",
    "        sync_index = X[sync_mask].index\n",
    "\n",
    "        labels = [int(X.loc[idx + 1, self.label_col]) for idx in sync_index[:-1]]\n",
    "\n",
    "        bounds = np.array([])\n",
    "\n",
    "        for l, r in zip(sync_index, sync_index[1:]):\n",
    "            bounds = np.append(bounds, np.argmax(peaks[l: r]) + l)\n",
    "\n",
    "        X_mrk = X.copy()\n",
    "        X_mrk[TARGET] = 0\n",
    "\n",
    "        for i, lr in enumerate(zip(bounds, np.append(bounds[1:], X_mrk.index[-1] + 1))):\n",
    "            l, r = lr\n",
    "            # l, r - индексы начала текущего и следующего жестов соответственно\n",
    "            X_mrk.loc[l: r, TARGET] = labels[i]\n",
    "\n",
    "        return X_mrk, bounds + self.bounds_shift\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "\n",
    "        # 1. Определим параметры монтажа:\n",
    "\n",
    "        grouped = X[X[LABEL_COL] != LABELS['Neutral']].groupby(self.sync_col)\n",
    "        # - периодичность измерений – разность между соседними метками времени\n",
    "        ts_delta = np.median((X[self.ts_col].shift(-1) - X[self.ts_col]).value_counts().index)\n",
    "\n",
    "        # - среднее кол-во измерений на один (не нейтральный) жест\n",
    "        self.ticks_per_gest = int(\n",
    "            grouped[self.ts_col].count().median()\n",
    "        )\n",
    "\n",
    "        # - среднее кол-во измерений на один разделительный нейтральный жест\n",
    "        self.ticks_per_nogo = int(\n",
    "            ((grouped[self.ts_col].first() - grouped[self.ts_col].first().shift(1)) / ts_delta).median() - self.ticks_per_gest\n",
    "        )\n",
    "\n",
    "        # 2. Определим датчики с высоким уровнем сигнала\n",
    "\n",
    "        omg_medians = pd.DataFrame(\n",
    "            MinMaxScaler().fit_transform(pd.DataFrame(X[OMG_CH].median(axis=0))),\n",
    "            index=OMG_CH, columns=['omg']\n",
    "        )\n",
    "        self.hi_val_sensors = omg_medians[omg_medians['omg'] > self.hi_val_threshold].index.to_list()\n",
    "\n",
    "        # 3. Исключим датчики с внезапными падениями сигнала\n",
    "        # (используем для этого заданный порог отношения первого процентиля к медиане)\n",
    "\n",
    "        # По каждому из активных датчиков посчитаем отношение первого процентиля к медиане\n",
    "        q_to_med = pd.Series(\n",
    "            X[self.hi_val_sensors].quantile(0.01) / X[self.hi_val_sensors].median(),\n",
    "            index=self.hi_val_sensors\n",
    "        )\n",
    "        # Отфильтруем датчики по заданному порогу self.sudden_drop_threshold\n",
    "        sudden_drop_sensors = q_to_med[q_to_med <= self.sudden_drop_threshold].index\n",
    "        self.mark_sensors = [sensor for sensor in self.hi_val_sensors if sensor not in sudden_drop_sensors]\n",
    "\n",
    "        # 4. Исключим датчики с перегрузкой\n",
    "\n",
    "        # Сколько идущих подряд максимальных значений считать перегрузкой\n",
    "        in_a_row_threshold = 5\n",
    "        # Доля перегруженного сигнала, чтобы исключить датчик из определения границ\n",
    "        clip_threshold = 0.05\n",
    "\n",
    "        clip_sensors = []\n",
    "\n",
    "        # Для каждого из рассматриваемых датчиков найдем его максимум\n",
    "        for sensor in self.mark_sensors:\n",
    "            mask = X[sensor] == X[sensor].max()\n",
    "            in_a_row = []\n",
    "            cur = 0\n",
    "            for x in mask:\n",
    "                if not x:\n",
    "                    if cur >= in_a_row_threshold:\n",
    "                        in_a_row.append(cur)\n",
    "                    cur = 0\n",
    "                else:\n",
    "                    cur += 1\n",
    "            if cur >= in_a_row_threshold:\n",
    "                in_a_row.append(cur)\n",
    "            if sum(in_a_row) / X.shape[0] > clip_threshold:\n",
    "                clip_sensors.append(sensor)\n",
    "\n",
    "        if len(clip_sensors):\n",
    "            self.mark_sensors = [sensor for sensor in self.mark_sensors if sensor not in clip_sensors]\n",
    "\n",
    "        # Теперь у нас готов список датчиков self.mark_sensors, по которым мы и будем определять границы жестов\n",
    "\n",
    "        # Установим ширину окна (для сглаживания медианой)\n",
    "        self.window = self.ticks_per_gest // 3\n",
    "        # и параметр spacing для вычисления градиентов\n",
    "        self.spacing = self.ticks_per_gest // 3\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None):\n",
    "        if self.label_col in X.columns:\n",
    "            X_marked, _ = self._mark(X)\n",
    "            return X_marked\n",
    "        else:\n",
    "            return X.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# Класс для создания лаговых признаков\n",
    "class LagFeaturesMaker(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_lags: int = 3\n",
    "    ):\n",
    "        self.n_lags = n_lags\n",
    "        self.X_flt = np.empty((0,))\n",
    "\n",
    "    def transform(self, X: np.ndarray or pd.DataFrame, y=None):\n",
    "\n",
    "        X = np.array(X)\n",
    "\n",
    "        m = X.shape[1] if len(X.shape) == 2 else X.shape[0]  # количество исходных признаков\n",
    "        w = m * self.n_lags  # количество лаговых признаков\n",
    "\n",
    "        # Если наш объект получает данные впервые,\n",
    "        if self.X_flt.shape[0] == 0:\n",
    "            # накопируем первый пример\n",
    "            self.X_flt = np.tile(X[0], self.n_lags - 1)\n",
    "\n",
    "        self.X_flt = np.hstack((self.X_flt, X.flatten()))\n",
    "\n",
    "        X_lag = np.vstack(\n",
    "            [self.X_flt[i: i + w] for i in range(0, self.X_flt.shape[0] - w + 1, m)]\n",
    "        )\n",
    "\n",
    "        # Запомним в нашем объекте лаги только для последнего примера\n",
    "        self.X_flt = self.X_flt[- (self.n_lags - 1) * m:]\n",
    "\n",
    "        return X_lag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания последовательностей\n",
    "def create_sequences(data, labels, timesteps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - timesteps + 1):\n",
    "        X.append(data[i:i + timesteps])\n",
    "        y.append(labels[i + timesteps - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Функция для создания последовательностей и кодирования меток\n",
    "def prepare_sequences(X_train_array, X_test_array, y_train_array, y_test_array, timesteps):\n",
    "    # Проверка и преобразование в np.ndarray\n",
    "    if not isinstance(X_train_array, np.ndarray):\n",
    "        X_train_array = np.array(X_train_array)\n",
    "    if not isinstance(y_train_array, np.ndarray):\n",
    "        y_train_array = np.array(y_train_array)\n",
    "    if not isinstance(X_test_array, np.ndarray):\n",
    "        X_test_array = np.array(X_test_array)\n",
    "    if not isinstance(y_test_array, np.ndarray):\n",
    "        y_test_array = np.array(y_test_array)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_array, y_train_array, timesteps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_array, y_test_array, timesteps)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_encoded = encoder.fit_transform(y_train_seq.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "    return X_train_seq, X_test_seq, y_train_encoded, y_test_encoded, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для оценки модели с использованием кросс-валидации\n",
    "def build_evaluate_model(timesteps, X_train_seq, y_train_encoded, X_test_seq, y_test_encoded):\n",
    "\n",
    "    # Построение модели\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(timesteps, X_train_seq.shape[2]), return_sequences=True),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.05),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.05),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        LayerNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Кросс-валидация\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_seq):\n",
    "        X_train_fold, X_val_fold = X_train_seq[train_index], X_train_seq[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_encoded[train_index], y_train_encoded[val_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=15, batch_size=32, verbose=0)\n",
    "        val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Average validation accuracy: {np.mean(accuracies)}\")\n",
    "\n",
    "    # Обучение модели на всех тренировочных данных\n",
    "    model.fit(X_train_seq, y_train_encoded, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "    # Оценка на тестовых данных\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_seq, y_test_encoded, verbose=0)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "    return np.mean(accuracies), test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_marked, last_train_idx=5500):\n",
    "    X = data_marked.drop(['act_label', 'id', 'ts', 'sample'], axis=1)\n",
    "    y = data_marked['act_label']\n",
    "\n",
    "    X_train = X[OMG_CH].values[:last_train_idx]\n",
    "    y_train = y['act_label'].values[:last_train_idx]\n",
    "\n",
    "    X_test = X[OMG_CH].values[last_train_idx:]\n",
    "    y_test = y['act_label'].values[last_train_idx:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_timesteps(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        timesteps_values: list = [1, 2, 3],\n",
    "        best_timesteps: int = None,\n",
    "        best_val_accuracy: float = 0,\n",
    "        best_test_accuracy: float = 0,\n",
    "\n",
    "):\n",
    "\n",
    "    for timesteps in timesteps_values:\n",
    "        # Создание последовательностей и кодирование меток\n",
    "        X_train_seq, X_test_seq, y_train_encoded, y_test_encoded, encoder = prepare_sequences(\n",
    "                                                                                            X_train, X_test, y_train, y_test, timesteps\n",
    "                                                                                        )\n",
    "        val_accuracy, test_accuracy = build_evaluate_model(timesteps, X_train_seq, y_train_encoded, X_test_seq, y_test_encoded)\n",
    "        print(f\"Timesteps: {timesteps}, Validation Accuracy: {val_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_timesteps = timesteps\n",
    "\n",
    "    # print(f\"Best Timesteps: {best_timesteps}, Best Validation Accuracy: {best_val_accuracy}, Best Test Accuracy: {best_test_accuracy}\")\n",
    "\n",
    "    return(best_timesteps)\n",
    "\n",
    "# validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(X_train_seq, y_train_encoded, best_timesteps=3):\n",
    "    # Построение модели\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(best_timesteps, X_train_seq.shape[2]), return_sequences=True),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        LayerNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Обучение модели на всех тренировочных данных\n",
    "    model.fit(X_train_seq, y_train_encoded, epochs=20, batch_size=32, verbose=0)\n",
    "    \n",
    "    model.save(MODEL_PATH)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для предсказания жеста руки\n",
    "def predict(model, X_test_seq):\n",
    "    y_pred_encoded = model.predict(X_test_seq)\n",
    "    #y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "    return y_pred_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Ячейка обучения \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'act_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m marker\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m      8\u001b[0m data_marked, bounds \u001b[38;5;241m=\u001b[39m marker\u001b[38;5;241m.\u001b[39m_mark(data)\n\u001b[1;32m---> 10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_marked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#best_timesteps = validate_timesteps(X_train, X_test, y_train, y_test)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m best_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36msplit_data\u001b[1;34m(data_marked, last_train_idx)\u001b[0m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data_marked[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mact_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X[OMG_CH]\u001b[38;5;241m.\u001b[39mvalues[:last_train_idx]\n\u001b[1;32m----> 6\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mact_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[:last_train_idx]\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X[OMG_CH]\u001b[38;5;241m.\u001b[39mvalues[last_train_idx:]\n\u001b[0;32m      9\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mact_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[last_train_idx:]\n",
      "File \u001b[1;32mc:\\Users\\User\\ANACONDA\\envs\\motorica_gestures\\lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\User\\ANACONDA\\envs\\motorica_gestures\\lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\User\\ANACONDA\\envs\\motorica_gestures\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'act_label'"
     ]
    }
   ],
   "source": [
    "montage = 'new/2024-12-09_11-22-43.emg8'\n",
    "\n",
    "data = read_emg8(montage, sep=' ')\n",
    "\n",
    "marker = BasePeakMarker(bounds_shift=-2)\n",
    "marker.fit(data)\n",
    "\n",
    "data_marked, bounds = marker._mark(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(data_marked)\n",
    "\n",
    "#best_timesteps = validate_timesteps(X_train, X_test, y_train, y_test)\n",
    "best_timesteps=1\n",
    "\n",
    "X_train_seq, X_test_seq, y_train_encoded, y_test_encoded, encoder = prepare_sequences(\n",
    "    X_train, X_test, y_train, y_test, best_timesteps\n",
    ")\n",
    "\n",
    "model = make_lstm_model(X_train_seq, y_train_encoded, best_timesteps=best_timesteps)\n",
    "\n",
    "y_pred = predict(model, X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1100, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_encoded = predict(model, X_test_seq)\n",
    "y_pred_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для обучения на нескольких монтажах\n",
    "```py\n",
    "# Инициализация списков для хранения данных\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "# Загрузка и предобработка данных для каждого монтажа\n",
    "for montage in montages:\n",
    "    data = read_emg8(montage)\n",
    "    \n",
    "    marker = BasePeakMarker(bounds_shift=-2)\n",
    "    marker.fit(data)\n",
    "\n",
    "    data_marked, bounds = marker._mark(data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(data_marked)\n",
    "\n",
    "    X_train_list.append(X_train_scaled)\n",
    "    X_test_list.append(X_test_scaled)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)\n",
    "```\n",
    "\n",
    "```py\n",
    "# Объединение данных\n",
    "X_train_combined = pd.concat(X_train_list, axis=0).reset_index(drop=True)\n",
    "X_test_combined = pd.concat(X_test_list, axis=0).reset_index(drop=True)\n",
    "y_train_combined = pd.concat(y_train_list, axis=0).reset_index(drop=True)\n",
    "y_test_combined = pd.concat(y_test_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Преобразование данных в массивы numpy\n",
    "X_train_array = X_train_combined.values\n",
    "y_train_array = y_train_combined.values\n",
    "X_test_array = X_test_combined.values\n",
    "y_test_array = y_test_combined.values\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motorica_gestures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
